<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>6 Alerting Strategies</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body class="bg-light">
  <div class="container my-5">
    <h1 class="mb-4">6 Alerting Strategies Overview</h1>

    <p>When evaluating an alerting strategy, we need to consider the following attributes:</p>
    <ul>
      <li><strong>Precision</strong> – Precision is 100% if every alert corresponds to a real issue (i.e., no false positives).</li>
      <li><strong>Recall</strong> – Recall is 100% if every significant issue triggers an alert (i.e., no false negatives).</li>
      <li><strong>Detection Time</strong> – How long it takes to send a notification after a significant issue begins.</li>
      <li><strong>Reset Time</strong> – How long it takes for the alert to stop firing after the issue is resolved.</li>
    </ul>

    <h2 class="mt-5">Target Error Rate &ge; SLO Threshold</h2>
    <p>Example: If the SLO is 99.9% over 30 days, alert if the error rate over the previous 10 minutes is &ge; 0.1%.</p>
    <p>Fires on any event that threatens the SLO.</p>
    <p><strong>Precision is low:</strong> For a short window (e.g., 10 minutes), an alert may fire for consuming just 0.02% of the monthly error budget—potentially too sensitive.</p>

    <h2 class="mt-5">Increased Alert Window</h2>
    <p>Increasing the window size means more of the error budget is consumed before the alert fires.</p>
    <p>Example: Alert if 5% of the 30-day error budget is consumed over a 36-hour window.</p>
    <p>Detection time is calculated as:</p>
    <pre><code>((1 - SLO) / error ratio) * window size</code></pre>
    <p>Improves precision, as the error must be sustained.</p>
    <ul>
      <li>Detection time is longer.</li>
      <li>Alerts fire shortly after an event starts and continue as long as the error persists.</li>
      <li>Downside: More expensive to compute due to larger data windows.</li>
    </ul>

    <h2 class="mt-5">Incrementing Alert Duration</h2>
    <p>Many monitoring tools allow setting a duration before firing an alert. The condition must be sustained over this time.</p>
    <ul>
      <li>Increases precision.</li>
      <li>Reduces recall and detection time, especially if the incident escalates quickly.</li>
      <li>If the metric returns to within SLO, the timer resets.</li>
    </ul>

    <h2 class="mt-5">Alert on Burn Rate</h2>
    <p>A burn rate represents how quickly the service consumes its error budget relative to the allowed rate.</p>
    <ul>
      <li>Good for fast detection with high precision.</li>
      <li>Allows shorter windows while keeping budget-consumption thresholds meaningful.</li>
    </ul>
    <p>Example: Burn rate = 2 → error budget lasts 15 days instead of 30.</p>
    <p>Alert fires after:</p>
    <pre><code>((1 - SLO) / error ratio) * window size * burn rate</code></pre>
    <p>Budget consumed when alert fires:</p>
    <pre><code>(burn rate * window size) / total period</code></pre>
    <p><strong>Pros:</strong></p>
    <ul>
      <li>Good precision</li>
      <li>Short detection time</li>
      <li>Efficient to compute (short window)</li>
    </ul>
    <p><strong>Cons:</strong></p>
    <ul>
      <li>Low recall: A 35x burn rate could use up the entire 30-day error budget in 20.5 hours without alerting.</li>
      <li>Reset time may still be long (e.g., 58 minutes).</li>
    </ul>

    <h2 class="mt-5">Multiple Burn Rate Alerts</h2>
    <p>Set up several burn rate thresholds and time windows to improve coverage and responsiveness.</p>
    <p>Examples:</p>
    <ul>
      <li>2% of budget in 1 hour</li>
      <li>5% in 6 hours</li>
      <li>10% in 3 days</li>
    </ul>
    <p><strong>Pros:</strong></p>
    <ul>
      <li>Flexibility to escalate based on severity</li>
      <li>Good precision</li>
      <li>Improved recall</li>
      <li>Tailored alerting per use case</li>
    </ul>
    <p><strong>Cons:</strong></p>
    <ul>
      <li>More thresholds = more management overhead</li>
      <li>May require alert deduplication/suppression</li>
    </ul>

    <h2 class="mt-5">Multiwindow, Multi-Burn-Rate Alerts</h2>
    <p>Improve alert quality by adding a short confirmation window to verify the issue is still occurring.</p>
    <p>Rule of thumb: short window = 1/12 of long window.</p>
    <p>Example: Page-level alert when burn rate is &gt;14x over both 1 hour and 5 minutes.</p>
    <p>Consumes ~2% of error budget.</p>
    <p><strong>Pros:</strong></p>
    <ul>
      <li>Alerts correspond better to ongoing incidents</li>
      <li>High precision and recall</li>
    </ul>
    <p><strong>Cons:</strong></p>
    <ul>
      <li>Complex configuration</li>
      <li>Harder to manage</li>
    </ul>

    <h2 class="mt-5">Overall</h2>
    <p>Multi-burn-rate alerts work best for high-traffic services, offering fast and precise alerts.</p>
    <p>In low-traffic services, they may generate false alarms or miss real issues due to insufficient data.</p>
    <p><strong>Strategies for low-traffic services:</strong></p>
    <ul>
      <li>Generate artificial traffic (e.g., synthetic tests).</li>
      <li>Group small services for collective monitoring.</li>
    </ul>

  </div>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
